import numpy as np

class MetricsCalculator:
    def __init__(self):
        self.confusion_metrics = None

    '''
            :param self
            :param test data - data which has actual outcome
            :param predicted_values - predicted output generated by model 
            :return accuracy
    '''
    def calculate_accuracy(self, test_data, predicted_values):
        prediction_value_index = test_data.shape[1] - 1
        correct_predictions = 0

        for i in range(0, len(predicted_values)):
            if int(test_data[i][prediction_value_index]) == predicted_values[i]:
                correct_predictions += 1

        accuracy = (correct_predictions/len(predicted_values))*100
        return accuracy

    '''
        :param self
        :param test data - data which has actual outcome
        :param predicted_values - predicted output generated by model 
        :return confusion matrix
    '''
    def create_confusion_matrix(self, test_data, predicted_values):
        matrix = np.zeros((2,2))
        actual_value_index = test_data.shape[1] - 1

        # 1 is positive
        # 0 is negative

        for i in range(0, len(predicted_values)):
            if predicted_values[i] == 1:
                if int(test_data[i][actual_value_index]) == 1:
                    matrix[0][0] += 1   # True positives
                else:
                    matrix[0][1] += 1   # False positives
            else:
                if int(test_data[i][actual_value_index]) == 0:
                    matrix[1][1] += 1   #True Negatives
                else:
                    matrix[1][0] += 1   #False Negatives
        if matrix[0][0] == 0 or matrix[0][1] == 0 or matrix[1][0] == 0 or matrix[1][1] == 0:
            matrix[0][0] += 1
            matrix[0][1] += 1
            matrix[1][0] += 1
            matrix[1][1] += 1

        self.confusion_metrics = matrix


    '''
        :param self
        :param test data - data which has actual outcome
        :param predicted_values - predicted output generated by model 
        :return precision
    '''
    def calculate_precision(self, test_data, predicted_values):
        if self.confusion_metrics is None:
            self.create_confusion_matrix(test_data, predicted_values)

        return self.confusion_metrics[0][0] / (self.confusion_metrics[0][0] + self.confusion_metrics[0][1])

    '''
        :param self
        :param test data - data which has actual outcome
        :param predicted_values - predicted output generated by model 
        :return recall
    '''
    def calculate_recall(self, test_data, predicted_values):
        if self.confusion_metrics is None:
            self.create_confusion_matrix(test_data, predicted_values)

        return self.confusion_metrics[0][0] / (self.confusion_metrics[0][0] + self.confusion_metrics[1][0])

    '''
        :param self
        :param precision
        :param recall 
        :return F1_score
    '''
    def calculate_F1_score(self, precision, recall):
        return 2 * ((precision * recall) / (precision + recall))